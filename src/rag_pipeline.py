"""
Core RAG logic steered / run by the following components:
- RAGPipeline:
    - sits in middle -> only component talking to App / FE and LLM
    - instanciates RAGConfig -> TokenManager -> RAGEngine
    - passes these instances further & manages state resets -> all components have same state
- App / FE: passes user prompt to RAGPipeline, gets back RAG-enriched LLM response
- RAGEngine:
    - receives user prompt from RAGPipeline, passes back RAG content for LLM call
    - instanciates VectorDB -> only component talking to VectorDB
- VectorDB: ChromaDB client for cosine similarity and text search
- TokenManager: calculates & reports state of available tokens to RAGEngine & RAGPipeline
- RAGConfig: provides RAG / token / llm parameters to all other components
"""

import os
import re
import json
from dotenv import load_dotenv
from langchain_groq import ChatGroq
from src.token_manager import TokenManager
from src.vector_db import DB
from typing import List, Tuple, Dict
from src.config import RAGConfig
import groq

# load env variables from .env file
load_dotenv()
# get huggingface token
groq_api_key = os.getenv('GROQ_API_KEY')
if not groq_api_key:
    raise ValueError("GROQ_API_KEY not found in .env file.")


class RAGEngine:
    """
    - core rag logic with config & tokenmanager instances passed through by RAGPipeline
    - rag context is generated by direct text search and cosine similarity search
    - relationship boost is applied to entities which were referenced in relevant articles
    - init rag token budget -> direct text search -> cosine similarity search -> return rag str
    """
    def __init__(self, config: RAGConfig, tm: TokenManager, db=None):
        self.config = config
        self.tm = tm
        # connect to chromadb in read only mode
        self.db = db if db is not None else DB.read_mode()
        self.entities = self.db.entities
        # collect id's of entities added to rag context to prevent duplicates
        self.used_ids = set()
        # collect metadata from top 3 articles and direct matches to apply relationship boost on
        self.rel_boost_ids = set()
        # collect rag_context until passing it to RAGPipeline at end of operation
        self.rag_context = ""

    def execute(self, user_prompt: str) -> str:
        # calc token budget for rag context ops at start of executing RAGEngine
        self.tm.rag_ops_tokens = self.tm.rag_context_tokens
        self._find_direct_matches(user_prompt)
        self._find_semantic_matches(user_prompt)
        # update tokenmanager with effective consumed tokens by final rag output (not ops counter!)
        self.tm.remaining_tokens -= self.tm.get_token_amount(self.rag_context)
        return self.rag_context

    def _find_direct_matches(self, user_prompt: str) -> None:
        """
        - scans user prompt for direct references to articles, annexes, or recitals
        - supports case-insensitive whole-word matches like "Article 5", "annex 12", "Recital 18"
        - collects up to 3 unique valid matches in left-to-right order
        - queries DB for exact IDs, adds to rag_context with distance 0.0 (= 100 % match)
        - updates used_ids and remaining_rag_tokens
        """
        # search configs with ranges and padding per entity
        search_configs = self.config.search_configs
        # regex to find matches: whole words, case-insensitive
        pattern = rf'\b({"|".join(self.config.search_configs.keys())})\s+(\d+)\b'
        matches = re.findall(pattern, user_prompt, re.IGNORECASE)
        # collect up to 3 unique candidates; if more than 3, take first 3 from left to right
        candidates = []
        seen_ids = set()
        for entity_type, num_str in matches:
            entity_type = entity_type.lower()
            num = int(num_str)
            config = search_configs[entity_type]
            # check for invalid number ranges
            if num < 1 or num > config["max_num"]:
                continue
            padded_num = f"{num:0{config['pad_digits']}d}"
            entity_id = f"{config['id_prefix']}{padded_num}"
            if entity_id not in seen_ids:
                candidates.append((entity_id, config["collection"]))
                seen_ids.add(entity_id)
            if len(candidates) >= 3:
                break
        # query and add valid matches
        for entity_id, collection_name in candidates:
            collection = self.db.collections[collection_name]
            result = collection.get(ids=[entity_id])
            if not result["documents"]:
                print(f"Warning: ID {entity_id} not found in {collection_name}")
                continue
            # construct rag item in final format for llm to count tokens accurate; 0.0 for distance
            rag_item = self._format_rag_context((result["documents"][0], 0.0))
            # safety check with token count of rag item
            tokens = self.tm.get_token_amount(rag_item)
            if tokens > self.tm.rag_ops_tokens:
                break
            # add text content with distance 0.0 as "perfect match" signal
            self.rag_context += rag_item
            self.used_ids.add(entity_id)
            # if entity is article: add metadata to article relationships to boost them
            if collection_name == "articles":
                self.rel_boost_ids.update(self._extract_related_ids(result["metadatas"][0]))
            # update rag operation tokens
            self.tm.rag_ops_tokens -= tokens

    def _find_semantic_matches(self, user_prompt: str) -> List[Tuple[str, float]]:
        """
        - get top nearest entries user_prompt across all entities
        - always grab top 3 nearest articles as "central hub" + their relationships
        - if top 3 articles / direct search hits contain references to other entitites -> boost them
        - fill up top relations (minus the 3 already added articles) with entities over a
          relevance threshold until rag token limit reached
        """
        # query db for nearest entities (different amounts depending on entity type: -> config)
        nearest_entries = {
            entity: self.db.collections[entity].query(
                query_texts=[user_prompt],
                n_results=getattr(self.config, f"nearest_{entity}"),
            )
            for entity in self.entities
        }
        # BASE RAG CONTEXT: always add top n articles (independend of cos similarity)
        for i in range(min(self.config.base_articles, len(nearest_entries["articles"]["ids"][0]))):
            # fetch needed data to execute safety check first with token count
            text_content = nearest_entries["articles"]["documents"][0][i]
            distance = nearest_entries["articles"]["distances"][0][i]
            # construct rag item in final format for llm to count tokens accurate
            rag_item = self._format_rag_context((text_content, distance))
            # safety check with text_content of current article, before appending
            tokens = self.tm.get_token_amount(rag_item)
            if tokens > self.tm.rag_ops_tokens:
                break
            # fetch the rest of data only if safety check went through
            article_id = nearest_entries["articles"]["ids"][0][i]
            metadata = nearest_entries["articles"]["metadatas"][0][i]
            # append / add data to containers & update token budget
            self.rag_context += rag_item
            self.used_ids.add(article_id)
            self.rel_boost_ids.update(self._extract_related_ids(metadata))
            self.tm.rag_ops_tokens -= tokens

        # create candidates of nearest entities: boost; filter for relevance; prevent duplicates
        candidates = []
        for entity in self.entities:
            candidate = nearest_entries[entity]
            for item_id, dist, doc in zip(
                candidate["ids"][0], candidate["distances"][0], candidate["documents"][0]
            ):
                # check if item gets relationship boost
                if item_id in self.rel_boost_ids:
                    dist *= self.config.relationship_boost
                # filtering: no duplicates; only entities with over relevance threshold
                if item_id not in self.used_ids and dist < self.config.rel_threshold:
                    candidates.append({"content": doc, "distance": dist})

        # ADDITIONAL RAG CONTEXT: fill up remaining space as long suited candidates are available
        for candidate in sorted(candidates, key=lambda x: x["distance"]):
            rag_item = self._format_rag_context((candidate["content"], candidate["distance"]))
            tokens = self.tm.get_token_amount(rag_item)
            if tokens > self.tm.rag_ops_tokens:
                break
            self.rag_context += rag_item
            self.tm.rag_ops_tokens -= tokens

    @staticmethod
    def _extract_related_ids(article_meta: Dict) -> set:
        """
        - extract relationship ids from one article metadata dict as input & return set of ids
        - read out from json str; there are no relationships to definitons entity
        """
        related_ids = set()
        rel_entities = ["articles", "recitals", "annexes"]
        for entity in rel_entities:
            related_ids.update(json.loads(article_meta.get(f"related_{entity}", "[]")))
        return related_ids

    @staticmethod
    def _format_rag_context(rag_raw: Tuple[str, float]) -> str:
        """
        - takes (text_content, distance) tuples as input, returns in final str format
        - convert cosine distance from float to relevance percentage that LLM understands easier
        """
        return f"[Relevance: {((1 - rag_raw[1]) * 100):.0f}%]\n{rag_raw[0]}\n\n---\n\n"

    def _reset_state(self) -> None:
        """ resets all components of RAGEngine to enable further user queries in same session """
        self.used_ids = set()
        self.rel_boost_ids = set()
        self.rag_context = ""


class RAGPipeline:
    """
    - steering RAG process using classes RAGEngine, TokenManager and RAGConfig
    - flow:
        1. init RAG Pipeline with VectorDB, Tokenizer, ...
        2. report user_query_len to App / FE
        3. receive user prompt and trigger RAG context creation with it
        4. execute RAG-enriched LLM call
        5. deliver response to App / FE
    """
    def __init__(self, config=None, tm=None, rag_engine=None):
        # init config with default params / systemmessages
        self.cfg = config if config is not None else RAGConfig()
        # init tokenmanager with passing the config
        self.tm = tm if tm is not None else TokenManager(self.cfg)
        # init rag engine with instances of config & tokenmanager
        self.engine = rag_engine if rag_engine is not None else RAGEngine(self.cfg, self.tm)
        # rag context created by RAGEngine after user query is processed
        self.rag_context = ""
        # langchain model will be instanciated after rag context gen with llm response max_tokens
        self.model = None

    @property
    def user_query_len(self) -> int:
        """
        - tell the app / fe how many chars for user query derived from token manager
        - must be converted from tokens into chars: (4-5 chars/token, ~1 token/word)
        """
        return int(self.tm.user_query_tokens * 4)

    def _validate_user_prompt(self, user_prompt: str) -> bool:
        """ calc token amount of user input str; if valid len return True"""
        assert self.tm.get_token_amount(user_prompt) <= self.tm.user_query_tokens, "Too long prompt"
        assert isinstance(user_prompt, str), "Invalid user prompt data type."
        return True

    def _init_model(self, token_budget: int) -> None:
        """ init langchain model with token budget """
        model = ChatGroq(
            model=self.cfg.llm,
            max_retries=2,
            max_tokens=token_budget,
            temperature=0.7,
            groq_api_key=groq_api_key,
        )
        self.model = model

    def _query_llm(self, user_prompt: str, rag_enriched: bool = True) -> str:
        """ flag rag_enriched: switch for systemmessage & if rag context is added to llm query """
        if rag_enriched and not self.rag_context:
            raise ValueError("RAG enriched mode requires rag_context to be set")
        # case 1: without rag
        if not rag_enriched:
            messages_base = [
                ("system", self.cfg.system_message_rag_disabled),
                ("human", user_prompt)
            ]
        # case 2: with rag
        else:
            messages_base = [
                ("system", self.cfg.system_message_rag_enabled),
                ("human", f"""Retrieved EU AI Act content with relevance scores:
                {self.rag_context}
                Question: {user_prompt}""")
            ]
        try:
            return self.model.invoke(messages_base).content
        except groq.AuthenticationError as e:
            raise ValueError("Invalid API key; check .env or HF secrets.") from e
        except groq.APIStatusError as e:
            if e.status_code == 413:
                raise ValueError(f"Request too large: {e}") from e
            elif e.status_code == 429:
                raise ConnectionRefusedError(f"Rate limit exceeded: {e}") from e
            else:
                raise ConnectionRefusedError(f"Groq API error {e.status_code}: {e}") from e
        except groq.APITimeoutError as e:
            raise TimeoutError(f"Groq API timeout: {e}") from e
        except groq.APIConnectionError as e:
            raise ConnectionError(f"Groq API connection error: {e}") from e
        except Exception as e:
            raise RuntimeError(f"Unexpected error in LLM call: {type(e).__name__}: {e}") from e

    def _reset_state(self) -> None:
        """ reset all necessary components to enable further user querys in same session """
        self.tm.reset_state()
        self.engine._reset_state()
        self.rag_context = ""
        self.model = None

    def process_query(self, user_prompt: str, rag_enriched: bool = True):
        """
        - central method to process the user_prompt until generating llm response
        - always reset for fresh state at start to enable multiple user queries in app
        - updates remaining tokens / delivers budget to app / fe and llm; RAGEngine does by itsself
        """
        self._reset_state()
        # validate user prompt from app / fe
        if not self._validate_user_prompt(user_prompt):
            raise ValueError("RAG process aborted: too long user prompt or wrong data type")
        # reduce consumed tokens by user prompt
        self.tm.reduce_remaining_tokens(user_prompt)
        # retrieve raw rag_context
        self.rag_context = self.engine.execute(user_prompt=user_prompt)
        # init langchain model providing remaining tokens to specify max_tokens llm response
        self._init_model(token_budget=self.tm.llm_response_tokens)
        # create llm response prompt
        llm_response = self._query_llm(user_prompt=user_prompt, rag_enriched=rag_enriched)
        # testing print delete for production
        print(f"RAG Content: \n{self.rag_context}")
        print(f"BOOST_rels: \n{self.engine.rel_boost_ids}")
        return llm_response


def main():
    app = RAGPipeline()
    #print(f"MAX USER QUERY CHARS: {app.user_query_len}")
    prompt = "How do the requirements for AI regulatory sandboxes relate to innovation support for SMEs, and what about Article 2?"
    #print(f"USER PROMPT: \n {prompt}")
    print(f"LLM RESPONSE: \n{app.process_query(user_prompt=prompt, rag_enriched=True)}")


if __name__ == "__main__":
    main()
